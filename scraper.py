# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import yaml
import time
import random
import hashlib # Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙƒÙ…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† hash() Ø§Ù„Ù…Ø¨Ø§Ø´Ø±

# =========================================================
# âš ï¸ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹: Ù‚Ù… Ø¨ØªØ­Ø¯ÙŠØ« Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµØ­ÙŠØ­Ø©
# =========================================================

# 1. Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø·ÙˆÙŠÙ„Ø© (Landing URLs) Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø³Ø­Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†Ù‡Ø§ (ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† ØªØ±ØªÙŠØ¨Ù‡Ø§ Ù…Ø·Ø§Ø¨Ù‚Ø§Ù‹ Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©)
PRODUCT_URLS = [
    "Ø§Ù„Ø±Ø§Ø¨Ø·_Ø§Ù„Ø·ÙˆÙŠÙ„_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ_Ù„Ù„Ù…Ù†ØªØ¬_1_Ù…Ù†_Ø§Ù„Ù…ØªØµÙØ­", 
    "Ø§Ù„Ø±Ø§Ø¨Ø·_Ø§Ù„Ø·ÙˆÙŠÙ„_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ_Ù„Ù„Ù…Ù†ØªØ¬_2_Ù…Ù†_Ø§Ù„Ù…ØªØµÙØ­", 
    "Ø§Ù„Ø±Ø§Ø¨Ø·_Ø§Ù„Ø·ÙˆÙŠÙ„_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ_Ù„Ù„Ù…Ù†ØªØ¬_3_Ù…Ù†_Ø§Ù„Ù…ØªØµÙØ­",
    "Ø§Ù„Ø±Ø§Ø¨Ø·_Ø§Ù„Ø·ÙˆÙŠÙ„_Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ_Ù„Ù„Ù…Ù†ØªØ¬_4_Ù…Ù†_Ø§Ù„Ù…ØªØµÙØ­"
]

# 2. Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¥Ø­Ø§Ù„Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø© (Affiliate URLs) Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ÙˆØ¶Ø¹Ù‡Ø§ ÙÙŠ Ø²Ø± Ø§Ù„Ø´Ø±Ø§Ø¡ (ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† ØªØ±ØªÙŠØ¨Ù‡Ø§ Ù…Ø·Ø§Ø¨Ù‚Ø§Ù‹ Ù„Ù„Ø£ÙˆÙ„Ù‰)
AFFILIATE_URLS = [
    "https://s.click.aliexpress.com/e/_c4mrsRWb", 
    "https://s.click.aliexpress.com/e/_c3j3Tkft", 
    "https://s.click.aliexpress.com/e/_c3CVW26n",
    "https://s.click.aliexpress.com/e/_c3wXwMHz"
]

# =========================================================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'ar-EG,ar;q=0.9',
    'Referer': 'https://ar.aliexpress.com/'
}

scraped_deals = []

def scrape_product(url, affiliate_link):
    """Ø¯Ø§Ù„Ø© Ù„Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†ØªØ¬ ÙˆØ§Ø­Ø¯ØŒ Ù…Ø¹ Ø±Ø¨Ø·Ù‡Ø§ Ø¨Ø±Ø§Ø¨Ø· Ø§Ù„Ø¥Ø­Ø§Ù„Ø©."""
    try:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status() 
        
        soup = BeautifulSoup(response.content, 'html.parser')

        # Ù…Ù„Ø§Ø­Ø¸Ø©: Ù…Ø­Ø¯Ø¯Ø§Øª CSS Ù‚Ø¯ ØªØªØºÙŠØ±. ÙŠØ¬Ø¨ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡Ø§.
        title_element = soup.find('h1', class_='product-title-text')
        title = title_element.text.strip() if title_element else "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†"

        price_element = soup.find('div', class_='price-current')
        price = price_element.text.strip() if price_element else "0.00"
        
        image_meta = soup.find('meta', property='og:image')
        image_url = image_meta['content'] if image_meta and 'content' in image_meta.attrs else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø©"
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… hashlib Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ ÙˆØ«Ø§Ø¨Øª
        unique_id = hashlib.sha1(url.encode()).hexdigest()

        deal_data = {
            'id': unique_id, 
            'title_raw': title,
            'price_scraped': price,
            'image_url': image_url,
            'aliexpress_url': url,
            'exclusive_link': affiliate_link # Ø§Ù„Ø¢Ù† Ù†Ø³ØªØ®Ø¯Ù… Ø±Ø§Ø¨Ø· Ø§Ù„Ø¥Ø­Ø§Ù„Ø© Ø§Ù„ØµØ­ÙŠØ­
        }
        
        print(f"âœ… ØªÙ… Ø³Ø­Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬: {title}")
        return deal_data

    except requests.RequestException as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø±Ø§Ø¨Ø· {url}: {e}")
        return None
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª {url}: {e}")
        return None

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³Ø­Ø¨ ÙˆØ­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    
    # ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù‚Ø§Ø¦Ù…ØªØ§Ù† Ù…ØªØ³Ø§ÙˆÙŠØªÙŠÙ† ÙÙŠ Ø§Ù„Ø·ÙˆÙ„
    if len(PRODUCT_URLS) != len(AFFILIATE_URLS):
        print("âŒ Ø®Ø·Ø£: ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¹Ø¯Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³Ø­Ø¨ Ù…Ø³Ø§ÙˆÙŠØ§Ù‹ Ù„Ø¹Ø¯Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¥Ø­Ø§Ù„Ø©.")
        return

    # Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙÙ‡Ø±Ø³
    for i, url in enumerate(PRODUCT_URLS):
        affiliate_link = AFFILIATE_URLS[i]
        
        deal = scrape_product(url, affiliate_link)
        if deal:
            scraped_deals.append(deal)

        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø¨Ø·Ø¡ Ø¨ÙŠÙ† ÙƒÙ„ Ø·Ù„Ø¨ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        sleep_time = random.uniform(5, 15)
        print(f"ğŸ˜´ Ø§Ù†ØªØ¸Ø§Ø± Ù„Ù€ {sleep_time:.2f} Ø«Ø§Ù†ÙŠØ©...")
        time.sleep(sleep_time)

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù YAML (Ù„Ù€ Jekyll)
    output_file = '_data/deals.yml'
    with open(output_file, 'w', encoding='utf-8') as f:
        yaml.dump(scraped_deals, f, allow_unicode=True, sort_keys=False)
        
    print(f"\nğŸ‰ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. ØªÙ… Ø­ÙØ¸ {len(scraped_deals)} Ù…Ù†ØªØ¬ ÙÙŠ {output_file}")


if __name__ == "__main__":
    main()
